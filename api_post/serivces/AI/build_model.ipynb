{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6599,
     "status": "ok",
     "timestamp": 1683281972068,
     "user": {
      "displayName": "Bằng Phạm Hữu",
      "userId": "03114714703305998686"
     },
     "user_tz": -420
    },
    "id": "5kgNGXcnMfAA",
    "outputId": "0a7be940-5bbf-46d3-84d1-87c6f863294e",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1548,
     "status": "ok",
     "timestamp": 1683281973613,
     "user": {
      "displayName": "Bằng Phạm Hữu",
      "userId": "03114714703305998686"
     },
     "user_tz": -420
    },
    "id": "Pqku5hpeMZZS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger # thư viện NLP tiếng Việt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim # thư viện NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17749,
     "status": "ok",
     "timestamp": 1683281995400,
     "user": {
      "displayName": "Bằng Phạm Hữu",
      "userId": "03114714703305998686"
     },
     "user_tz": -420
    },
    "id": "kdIVfLxEWT8b",
    "outputId": "94c1691f-4f98-4576-d8ed-067597381dd9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"C:/Users/bangp/Downloads/VNTC-master.zip\", 'r')\n",
    "zip_ref.extractall(\"/content/DATA\")\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting patool\n",
      "  Using cached patool-1.12-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: patool\n",
      "Successfully installed patool-1.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install patool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import patoolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting /content/DATA/VNTC-master/Data/10Topics/Ver1.1/Train_Full.rar ...\n",
      "patool: running \"C:\\Program Files\\WinRAR\\rar.EXE\" x -- c:\\content\\DATA\\VNTC-master\\Data\\10Topics\\Ver1.1\\Train_Full.rar\n",
      "patool:     with cwd=/content/DATA\n",
      "patool: ... /content/DATA/VNTC-master/Data/10Topics/Ver1.1/Train_Full.rar extracted to `/content/DATA'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content/DATA'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patoolib.extract_archive(\"/content/DATA/VNTC-master/Data/10Topics/Ver1.1/Train_Full.rar\", outdir=\"/content/DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMmormY3bsXk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5219/5219 [00:35<00:00, 147.90it/s]\n",
      "100%|██████████| 3159/3159 [00:28<00:00, 109.53it/s]\n",
      "100%|██████████| 1820/1820 [00:13<00:00, 130.12it/s]\n",
      "100%|██████████| 2552/2552 [00:18<00:00, 136.37it/s]\n",
      "100%|██████████| 3868/3868 [00:43<00:00, 89.71it/s]\n",
      "100%|██████████| 3384/3384 [00:49<00:00, 69.05it/s]\n",
      "100%|██████████| 2898/2898 [00:38<00:00, 76.04it/s]\n",
      "100%|██████████| 5298/5298 [01:15<00:00, 69.72it/s]\n",
      "100%|██████████| 3080/3080 [00:44<00:00, 69.81it/s]\n",
      "100%|██████████| 2481/2481 [00:30<00:00, 81.23it/s]\n",
      "100%|██████████| 10/10 [06:18<00:00, 37.86s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    dirs = os.listdir(folder_path)\n",
    "    for path in tqdm(dirs):\n",
    "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
    "        for file_path in tqdm(file_paths):\n",
    "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
    "                lines = f.readlines()\n",
    "                lines = ' '.join(lines)\n",
    "                lines = gensim.utils.simple_preprocess(lines)\n",
    "                lines = ' '.join(lines)\n",
    "                lines = ViTokenizer.tokenize(lines)\n",
    "\n",
    "                X.append(lines)\n",
    "                y.append(path)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "train_path = os.path.join('c:\\content\\DATA', 'Train_Full')\n",
    "X_data, y_data = get_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8r5PpS2XYkdw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(X_data, open('c:\\content\\DATA\\X_data.pkl', 'wb'))\n",
    "pickle.dump(y_data, open('c:\\content\\DATA/y_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "s1ifracCcDfW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting /content/DATA/VNTC-master/Data/10Topics/Ver1.1/Test_Full.rar ...\n",
      "patool: running \"C:\\Program Files\\WinRAR\\rar.EXE\" x -- c:\\content\\DATA\\VNTC-master\\Data\\10Topics\\Ver1.1\\Test_Full.rar\n",
      "patool:     with cwd=/content/DATA\n",
      "patool: ... /content/DATA/VNTC-master/Data/10Topics/Ver1.1/Test_Full.rar extracted to `/content/DATA'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content/DATA'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patoolib.extract_archive(\"/content/DATA/VNTC-master/Data/10Topics/Ver1.1/Test_Full.rar\", outdir=\"/content/DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHVJrr9tbrh5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7567/7567 [02:07<00:00, 59.26it/s]\n",
      "100%|██████████| 2036/2036 [00:53<00:00, 37.95it/s]\n",
      "100%|██████████| 2096/2096 [00:42<00:00, 49.78it/s]\n",
      "100%|██████████| 5276/5276 [01:40<00:00, 52.43it/s]\n",
      "100%|██████████| 3788/3788 [01:40<00:00, 37.61it/s]\n",
      "100%|██████████| 5417/5417 [02:47<00:00, 32.34it/s]\n",
      "100%|██████████| 6716/6716 [03:25<00:00, 32.61it/s]\n",
      "100%|██████████| 6667/6667 [02:28<00:00, 44.86it/s]\n",
      "100%|██████████| 6250/6250 [02:01<00:00, 51.37it/s]\n",
      "100%|██████████| 4560/4560 [01:14<00:00, 61.41it/s]\n",
      "100%|██████████| 10/10 [19:04<00:00, 114.50s/it]\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join('c:\\content\\DATA', 'Test_Full')\n",
    "X_test, y_test = get_data(test_path)\n",
    "\n",
    "pickle.dump(X_test, open('c:\\content\\DATA/X_test.pkl', 'wb'))\n",
    "pickle.dump(y_test, open('c:\\content\\DATA/y_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_data = pickle.load(open('c:\\content\\DATA/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('c:\\content\\DATA/y_data.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('c:\\content\\DATA/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('c:\\content\\DATA/y_test.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\bangp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\bangp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\bangp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bangp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bangp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_data)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_data_count = count_vect.transform(X_data)\n",
    "X_test_count = count_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(X_data)\n",
    "X_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram-char level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram_char = TfidfVectorizer(analyzer='char', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram_char.fit(X_data)\n",
    "X_data_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)\n",
    "\n",
    "\n",
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_data_tfidf_ngram)\n",
    "\n",
    "X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n",
    "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram_char = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram_char.fit(X_data_tfidf_ngram_char)\n",
    "\n",
    "X_data_tfidf_ngram_char_svd = svd_ngram_char.transform(X_data_tfidf_ngram_char)\n",
    "X_test_tfidf_ngram_char_svd = svd_ngram_char.transform(X_test_tfidf_ngram_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors \n",
    "# dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "word2vec_model_path = os.path.join('c:\\content\\DATA', \"vi/vi.vec\")\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(word2vec_model_path)\n",
    "vocab = w2v.key_to_index\n",
    "wv = w2v\n",
    "\n",
    "def get_word2vec_data(X):\n",
    "    word2vec_data = []\n",
    "    for x in X:\n",
    "        sentence = []\n",
    "        for word in x.split(\" \"):\n",
    "            if word in vocab:\n",
    "                sentence.append(wv[word])\n",
    "\n",
    "        word2vec_data.append(sentence)\n",
    "\n",
    "    return word2vec_data\n",
    "\n",
    "X_data_w2v = get_word2vec_data(X_data)\n",
    "X_test_w2v = get_word2vec_data(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import  metrics\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_data_n = encoder.fit_transform(y_data)\n",
    "y_test_n = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
       "       'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
       "       'Vi tinh'], dtype='<U16')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "    return classifier\n",
    "        \n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8524881516587678\n",
      "Test accuracy:  0.8628034859944812\n"
     ]
    }
   ],
   "source": [
    "train_model(naive_bayes.MultinomialNB(), X_data_tfidf, y_data, X_test_tfidf, y_test, is_neuralnet=False)\n",
    "\n",
    "# kết quả\n",
    "# Train accuracy:  0.880031596616529\n",
    "# Validation accuracy:  0.8690758293838863\n",
    "# Test accuracy:  0.8650666031405714\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    layer = Dense(1024, activation='relu')(input_layer)\n",
    "    layer = Dense(1024, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.7637 - accuracy: 0.7735 - val_loss: 0.3354 - val_accuracy: 0.8827\n",
      "Epoch 2/3\n",
      "60/60 [==============================] - 3s 51ms/step - loss: 0.2604 - accuracy: 0.9131 - val_loss: 0.3005 - val_accuracy: 0.8969\n",
      "Epoch 3/3\n",
      "60/60 [==============================] - 3s 47ms/step - loss: 0.2275 - accuracy: 0.9237 - val_loss: 0.2837 - val_accuracy: 0.9067\n",
      "106/106 [==============================] - 1s 4ms/step\n",
      "1575/1575 [==============================] - 6s 4ms/step\n",
      "Validation accuracy:  0.9066943127962085\n",
      "Test accuracy:  0.9095944255851349\n"
     ]
    }
   ],
   "source": [
    "classifier = create_dnn_model()\n",
    "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brnn_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    \n",
    "    layer = Reshape((10, 30))(input_layer)\n",
    "    layer = Bidirectional(GRU(128, activation='relu'))(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60/60 [==============================] - 39s 637ms/step - loss: 1.4940 - accuracy: 0.5315 - val_loss: 0.8415 - val_accuracy: 0.7367\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 9s 141ms/step - loss: 0.5644 - accuracy: 0.8199 - val_loss: 0.5004 - val_accuracy: 0.8315\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 8s 126ms/step - loss: 0.4385 - accuracy: 0.8551 - val_loss: 0.4531 - val_accuracy: 0.8469\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 8s 130ms/step - loss: 0.4023 - accuracy: 0.8653 - val_loss: 0.4440 - val_accuracy: 0.8501\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 8s 127ms/step - loss: 0.3799 - accuracy: 0.8729 - val_loss: 0.4002 - val_accuracy: 0.8670\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 8s 141ms/step - loss: 0.3501 - accuracy: 0.8822 - val_loss: 0.3846 - val_accuracy: 0.8679\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 9s 154ms/step - loss: 0.3430 - accuracy: 0.8847 - val_loss: 0.3772 - val_accuracy: 0.8682\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 8s 127ms/step - loss: 0.3239 - accuracy: 0.8921 - val_loss: 0.3690 - val_accuracy: 0.8729\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 7s 125ms/step - loss: 0.3135 - accuracy: 0.8941 - val_loss: 0.3598 - val_accuracy: 0.8803\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 8s 126ms/step - loss: 0.3028 - accuracy: 0.8979 - val_loss: 0.3433 - val_accuracy: 0.8874\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 8s 130ms/step - loss: 0.2998 - accuracy: 0.8986 - val_loss: 0.3490 - val_accuracy: 0.8806\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 8s 131ms/step - loss: 0.2914 - accuracy: 0.9017 - val_loss: 0.3446 - val_accuracy: 0.8824\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 8s 129ms/step - loss: 0.2764 - accuracy: 0.9055 - val_loss: 0.3399 - val_accuracy: 0.8868\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 8s 126ms/step - loss: 0.2696 - accuracy: 0.9086 - val_loss: 0.3356 - val_accuracy: 0.8827\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 8s 125ms/step - loss: 0.2619 - accuracy: 0.9099 - val_loss: 0.3476 - val_accuracy: 0.8830\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 8s 128ms/step - loss: 0.2518 - accuracy: 0.9146 - val_loss: 0.3287 - val_accuracy: 0.8910\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 8s 133ms/step - loss: 0.2475 - accuracy: 0.9163 - val_loss: 0.3260 - val_accuracy: 0.8934\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 8s 141ms/step - loss: 0.2385 - accuracy: 0.9186 - val_loss: 0.3243 - val_accuracy: 0.8966\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 8s 129ms/step - loss: 0.2329 - accuracy: 0.9204 - val_loss: 0.3287 - val_accuracy: 0.8913\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 8s 132ms/step - loss: 0.2244 - accuracy: 0.9230 - val_loss: 0.3156 - val_accuracy: 0.8960\n",
      "106/106 [==============================] - 1s 8ms/step\n",
      "1575/1575 [==============================] - 9s 6ms/step\n",
      "Validation accuracy:  0.8960308056872038\n",
      "Test accuracy:  0.9047505608163103\n"
     ]
    }
   ],
   "source": [
    "classifier = create_brnn_model()\n",
    "model = train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "INFO:tensorflow:Assets written to: models/model_brnn\\assets\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier.save('models/model_brnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    \n",
    "    layer = Reshape((10, 30))(input_layer)\n",
    "    layer = Bidirectional(GRU(128, activation='relu', return_sequences=True))(layer)    \n",
    "    layer = Convolution1D(100, 3, activation=\"relu\")(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    classifier.summary()\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 10, 30)            0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 10, 256)          122880    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 8, 100)            76900     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               410112    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 939,502\n",
      "Trainable params: 939,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "60/60 [==============================] - 10s 143ms/step - loss: 1.6058 - accuracy: 0.4337 - val_loss: 0.7014 - val_accuracy: 0.7743\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 9s 144ms/step - loss: 0.5246 - accuracy: 0.8319 - val_loss: 0.4657 - val_accuracy: 0.8448\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 8s 132ms/step - loss: 0.4058 - accuracy: 0.8687 - val_loss: 0.4013 - val_accuracy: 0.8664\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 0.3666 - accuracy: 0.8796 - val_loss: 0.3899 - val_accuracy: 0.8691\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 7s 120ms/step - loss: 0.3410 - accuracy: 0.8892 - val_loss: 0.3612 - val_accuracy: 0.8768\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 7s 112ms/step - loss: 0.3144 - accuracy: 0.8962 - val_loss: 0.3438 - val_accuracy: 0.8839\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.2910 - accuracy: 0.9034 - val_loss: 0.3325 - val_accuracy: 0.8833\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.2787 - accuracy: 0.9056 - val_loss: 0.3543 - val_accuracy: 0.8827\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.2718 - accuracy: 0.9101 - val_loss: 0.3254 - val_accuracy: 0.8839\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.2561 - accuracy: 0.9130 - val_loss: 0.3351 - val_accuracy: 0.8868\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 6s 106ms/step - loss: 0.2537 - accuracy: 0.9148 - val_loss: 0.2978 - val_accuracy: 0.8969\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.2310 - accuracy: 0.9227 - val_loss: 0.3223 - val_accuracy: 0.8960\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.2246 - accuracy: 0.9230 - val_loss: 0.3008 - val_accuracy: 0.8981\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.2154 - accuracy: 0.9254 - val_loss: 0.3037 - val_accuracy: 0.8948\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.2037 - accuracy: 0.9304 - val_loss: 0.3090 - val_accuracy: 0.8954\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.1954 - accuracy: 0.9321 - val_loss: 0.3279 - val_accuracy: 0.8928\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.1862 - accuracy: 0.9358 - val_loss: 0.3301 - val_accuracy: 0.8925\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 6s 104ms/step - loss: 0.1759 - accuracy: 0.9395 - val_loss: 0.3123 - val_accuracy: 0.8948\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.1693 - accuracy: 0.9425 - val_loss: 0.3279 - val_accuracy: 0.8966\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 6s 104ms/step - loss: 0.1598 - accuracy: 0.9459 - val_loss: 0.3271 - val_accuracy: 0.8990\n",
      "106/106 [==============================] - 1s 3ms/step\n",
      "1575/1575 [==============================] - 5s 3ms/step\n",
      "Validation accuracy:  0.8989928909952607\n",
      "Test accuracy:  0.8992118793798265\n"
     ]
    }
   ],
   "source": [
    "classifier = create_rcnn_model()\n",
    "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFEW/IfoOc/ZtsuW4iTxNl",
   "mount_file_id": "1ZTo1hrAFJOdWfzd-n7sKtH_85zifeHMy",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
